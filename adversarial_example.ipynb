{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWd2WLe/7fSrj9XcwKntc3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# clone yolov5 repo\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qiSaa0aOPsK2","executionInfo":{"status":"ok","timestamp":1690915544495,"user_tz":240,"elapsed":30880,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"a48ad4ec-04c3-4be3-8857-535aa840a0bc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15898, done.\u001b[K\n","remote: Counting objects: 100% (130/130), done.\u001b[K\n","remote: Compressing objects: 100% (93/93), done.\u001b[K\n","remote: Total 15898 (delta 60), reused 77 (delta 37), pack-reused 15768\u001b[K\n","Receiving objects: 100% (15898/15898), 14.69 MiB | 5.74 MiB/s, done.\n","Resolving deltas: 100% (10872/10872), done.\n","/content/yolov5\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from models.common import DetectMultiBackend\n","from utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,\n","                           check_git_status, check_img_size, check_requirements, check_suffix, check_yaml, colorstr,\n","                           get_latest_run, increment_path, init_seeds, intersect_dicts, labels_to_class_weights,\n","                           labels_to_image_weights, methods, one_cycle, print_args, print_mutation, strip_optimizer,\n","                           yaml_save)\n","from utils.dataloaders import create_dataloader\n","from utils.loss import ComputeLoss\n","from utils.torch_utils import select_device, smart_inference_mode\n","from utils.metrics import bbox_iou\n","\n","from tqdm import tqdm\n","import torch\n","import yaml\n","IMAGE_SCALE = 2.0 / 255 # TODO: may need to tune this\n","import roboflow\n","from utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, select_device, smart_DDP, smart_optimizer,\n","                               smart_resume, torch_distributed_zero_first)\n"],"metadata":{"id":"ZxRh3CJzj_iY","executionInfo":{"status":"ok","timestamp":1690915650597,"user_tz":240,"elapsed":5189,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Dataset\n"],"metadata":{"id":"1wrX5leBbz7J"}},{"cell_type":"code","source":["from roboflow import Roboflow\n","rf = Roboflow(api_key=\"gMTVjr2tVu8feJmkrnPs\")\n","project = rf.workspace(\"aps360-project-6tmc3\").project(\"aps360_-2\")\n","dataset = project.version(7).download(\"yolov5\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xwD96e6dBKR","executionInfo":{"status":"ok","timestamp":1690915696116,"user_tz":240,"elapsed":38558,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"c01c350d-2f50-40ec-eb55-17e4cbf141e4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in APS360_(2)-7 to yolov5pytorch: 94% [102014976 / 107419411] bytes"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to APS360_(2)-7 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4583/4583 [00:01<00:00, 3273.09it/s]\n"]}]},{"cell_type":"markdown","source":["# Attacker Module"],"metadata":{"id":"jAku2sQIchm1"}},{"cell_type":"code","source":["# ref: https://github.com/amy-choi/AttackDefenseYOLO/blob/main/attacker.py\n","class FGSMAttacker:\n","    def __init__(self, epsilon, device, attack_type=\"total\"):\n","        self.epsilon = epsilon * IMAGE_SCALE\n","        self.device = device\n","        self.attack_type = attack_type\n","\n","    def attack(self, image_clean, label, yolo_loss, model):\n","        image_clean = image_clean.clone().detach().to(self.device)\n","        # generate an initial perturbation\n","        delta = (\n","            torch.zeros_like(image_clean)\n","            .uniform_(-self.epsilon, self.epsilon)\n","            .to(self.device)\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        delta.requires_grad = True\n","\n","        pred = model(image_clean + delta)\n","\n","\n","        loss_total, _, loss_loc, loss_obj, loss_cls = yolo_loss(\n","            pred, label\n","        )\n","\n","        if self.attack_type == \"cls\":\n","            loss = loss_cls\n","        elif self.attack_type == \"loc\":\n","            loss = loss_loc\n","        elif self.attack_type == \"conf\":\n","            loss = loss_obj\n","        elif self.attack_type == \"total\":\n","            loss = loss_total\n","        elif self.attack_type == \"conf_loc\":\n","            loss = loss_obj + loss_loc\n","        elif self.attack_type == \"conf_cls\":\n","            loss = loss_obj + loss_cls\n","        elif self.attack_type == \"cls_loc\":\n","            loss = loss_cls + loss_loc\n","\n","        loss.backward()\n","        grad = delta.grad.detach()\n","\n","        delta.data = torch.clamp(\n","            delta + self.epsilon * torch.sign(grad), -self.epsilon, self.epsilon\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        adv_img = (image_clean + delta).detach()\n","\n","        return adv_img\n","\n","\n","    def generate_cls(self, image_clean, label, yolo_loss, model):\n","        image_clean = image_clean.clone().detach().to(self.device)  # .cuda()\n","        delta = (\n","            torch.zeros_like(image_clean)\n","            .uniform_(-self.epsilon, self.epsilon)\n","            .to(self.device)\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        delta.requires_grad = True\n","\n","        outputs = model(image_clean + delta)\n","        losses = 0\n","        num_pos_all = 0\n","\n","        for l in range(len(outputs)):\n","            _, _, _, loss_cls, num_pos = yolo_loss(l, outputs[l], label)\n","            losses += loss_cls\n","            num_pos_all += num_pos\n","\n","        loss = losses / num_pos_all\n","\n","        loss.backward()\n","        grad = delta.grad.detach()\n","\n","        delta.data = torch.clamp(\n","            delta + self.epsilon * torch.sign(grad), -self.epsilon, self.epsilon\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        adv_img = (image_clean + delta).detach()\n","\n","        return adv_img\n","\n","    def generate_obj(self, image_clean, label, yolo_loss, model):\n","        image_clean = image_clean.clone().detach().to(self.device)  # .cuda()\n","        delta = (\n","            torch.zeros_like(image_clean)\n","            .uniform_(-self.epsilon, self.epsilon)\n","            .to(self.device)\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        delta.requires_grad = True\n","\n","        outputs = model(image_clean + delta)\n","        losses = 0\n","        num_pos_all = 0\n","\n","        for l in range(len(outputs)):\n","            _, _, loss_obj, _, num_pos = yolo_loss(l, outputs[l], label)\n","            losses += loss_obj\n","            num_pos_all += num_pos\n","\n","        loss = losses / num_pos_all\n","\n","        loss.backward()\n","        grad = delta.grad.detach()\n","\n","        delta.data = torch.clamp(\n","            delta + self.epsilon * torch.sign(grad), -self.epsilon, self.epsilon\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        adv_img = (image_clean + delta).detach()\n","\n","        return adv_img\n","\n","    def generate_loc(self, image_clean, label, yolo_loss, model):\n","        image_clean = image_clean.clone().detach().to(self.device)  # .cuda()\n","        delta = (\n","            torch.zeros_like(image_clean).uniform_(-self.epsilon, self.epsilon).cuda()\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        delta.requires_grad = True\n","\n","        outputs = model(image_clean + delta)\n","        losses = 0\n","        num_pos_all = 0\n","\n","        for l in range(len(outputs)):\n","            _, loss_loc, _, _, num_pos = yolo_loss(l, outputs[l], label)\n","            losses += loss_loc\n","            num_pos_all += num_pos\n","\n","        loss = losses / num_pos_all\n","\n","        loss.backward()\n","        grad = delta.grad.detach()\n","\n","        delta.data = torch.clamp(\n","            delta + self.epsilon * torch.sign(grad), -self.epsilon, self.epsilon\n","        )\n","        delta.data = torch.max(torch.min(1 - image_clean, delta.data), 0 - image_clean)\n","        adv_img = (image_clean + delta).detach()\n","\n","        return adv_img\n"],"metadata":{"id":"_0g_45-9chQn","executionInfo":{"status":"ok","timestamp":1690915696117,"user_tz":240,"elapsed":7,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Adapted Loss fn"],"metadata":{"id":"k8SI2bsrF2vC"}},{"cell_type":"code","source":["class ComputeIndividualLoss(ComputeLoss):\n","  def __init__(self, model, autobalance=False):\n","    super().__init__(model, autobalance)\n","\n","  # override the __call__ function to return the component loss tensors\n","  def __call__(self, p, targets):  # predictions, targets\n","    lcls = torch.zeros(1, device=self.device)  # class loss\n","    lbox = torch.zeros(1, device=self.device)  # box loss\n","    lobj = torch.zeros(1, device=self.device)  # object loss\n","    tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n","\n","    # Losses\n","    for i, pi in enumerate(p):  # layer index, layer predictions\n","        b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n","        tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj\n","\n","        n = b.shape[0]  # number of targets\n","        if n:\n","            # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0\n","            pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions\n","\n","            # Regression\n","            pxy = pxy.sigmoid() * 2 - 0.5\n","            pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i]\n","            pbox = torch.cat((pxy, pwh), 1)  # predicted box\n","            iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)\n","            lbox += (1.0 - iou).mean()  # iou loss\n","\n","            # Objectness\n","            iou = iou.detach().clamp(0).type(tobj.dtype)\n","            if self.sort_obj_iou:\n","              j = iou.argsort()\n","              b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]\n","            if self.gr < 1:\n","                iou = (1.0 - self.gr) + self.gr * iou\n","            tobj[b, a, gj, gi] = iou  # iou ratio\n","\n","            # Classification\n","            if self.nc > 1:  # cls loss (only if multiple classes)\n","                t = torch.full_like(pcls, self.cn, device=self.device)  # targets\n","                t[range(n), tcls[i]] = self.cp\n","                lcls += self.BCEcls(pcls, t)  # BCE\n","\n","            # Append targets to text file\n","            # with open('targets.txt', 'a') as file:\n","            #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\n","\n","        obji = self.BCEobj(pi[..., 4], tobj)\n","        lobj += obji * self.balance[i]  # obj loss\n","        if self.autobalance:\n","            self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\n","\n","    if self.autobalance:\n","        self.balance = [x / self.balance[self.ssi] for x in self.balance]\n","    lbox *= self.hyp['box']\n","    lobj *= self.hyp['obj']\n","    lcls *= self.hyp['cls']\n","    bs = tobj.shape[0]  # batch size\n","\n","    return (lbox + lobj + lcls), torch.cat((lbox, lobj, lcls)).detach(), lbox, lobj, lcls"],"metadata":{"id":"Izi7btEJF4zl","executionInfo":{"status":"ok","timestamp":1690915696117,"user_tz":240,"elapsed":5,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# adv examples"],"metadata":{"id":"Ml9zRTckfymL"}},{"cell_type":"code","source":["from utils.torch_utils import select_device, smart_inference_mode\n","from models.yolo import Model\n","from utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,\n","                           check_git_status, check_img_size, check_requirements, check_suffix, check_yaml, colorstr,\n","                           get_latest_run, increment_path, init_seeds, intersect_dicts, labels_to_class_weights,\n","                           labels_to_image_weights, methods, one_cycle, print_args, print_mutation, strip_optimizer,\n","                           yaml_save)\n","from utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, select_device, smart_DDP, smart_optimizer,\n","                               smart_resume, torch_distributed_zero_first)\n","\n","from utils.loss import ComputeLoss\n","import torch\n","import os\n","import yaml\n","LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n","RANK = int(os.getenv('RANK', -1))\n","WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\n","import numpy as np\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"f3gNKA5nWIdJ","executionInfo":{"status":"ok","timestamp":1690915696117,"user_tz":240,"elapsed":5,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"V9O7Uv2NPCxJ","executionInfo":{"status":"ok","timestamp":1690915696117,"user_tz":240,"elapsed":5,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"outputs":[],"source":["def _load_model(weights_path, device, nc, anchors):\n","  '''\n","    @param data_folder: path to the data.yaml file\n","    return a model that is loaded with the specified weights\n","  '''\n","  ckpt = torch.load(weights_path, map_location=\"cpu\")\n","  model = Model(ckpt['model'].yaml, ch=3, nc=nc, anchors=anchors).to(device)  # create\n","  exclude = []\n","  # exclude = ['anchor'] if anchors else []\n","  csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n","  csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n","  model.load_state_dict(csd, strict=False)  # load\n","  print(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights_path}')  # report)\n","  return model"]},{"cell_type":"code","source":["def get_adv_examples(epsilon,\n","                     weights_path,\n","                     num_batches,\n","                     att_type,\n","                     data_folder,\n","                     hyperparam_path,\n","                     batch_size=32,\n","                     imgsz=640,\n","                     device='',\n","                     seed=0,\n","                     workers=2,\n","                     label_smoothing=0.0):\n","  '''\n","      @param weights_path: path to weights of a model that you would like to generate\n","                            adversarial examples on\n","      @param data_folder: clean data\n","      @param att_type: # takes \"cls\", \"loc\", or \"conf\"\n","\n","  '''\n","  single_cls = False\n","  # load in the hyperparameters\n","  if isinstance(hyperparam_path, str):\n","        with open(hyperparam_path, errors='ignore') as f:\n","            hyp = yaml.safe_load(f)  # load hyps dict\n","\n","  cuda = device.type != 'cpu'\n","  init_seeds(seed + 1 + RANK, deterministic=True)\n","\n","  # load data dict\n","  with torch_distributed_zero_first(LOCAL_RANK):\n","      data_dict = check_dataset(data_folder)  # check if None\n","  nc = int(data_dict[\"nc\"])\n","  names = data_dict[\"names\"]\n","  is_coco = False\n","\n","  # load model with weights\n","  model = _load_model(weights_path, device, nc, hyp.get(\"anchors\"))\n","  amp = check_amp(model)  # check AMP\n","\n","  gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n","  imgsz = check_img_size(imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n","\n","  ema = ModelEMA(model) if RANK in {-1, 0} else None\n","\n","  train_path, val_path = data_dict['train'], data_dict['val']\n","  train_loader, dataset = create_dataloader(train_path,\n","                                            imgsz,\n","                                            batch_size // WORLD_SIZE,\n","                                            gs,\n","                                            single_cls,\n","                                            hyp=hyp,\n","                                            augment=False,\n","                                            cache=None,\n","                                            rect=False, #unless you specify this flag when training\n","                                            rank=LOCAL_RANK,\n","                                            workers=workers,\n","                                            image_weights=False, # unless you specify this flag when training\n","                                            quad=False, # unless you specify this flag when training\n","                                            prefix=colorstr('train: '),\n","                                            shuffle=True,\n","                                            seed=seed)\n","  labels = np.concatenate(dataset.labels, 0)\n","  mlc = int(labels[:, 0].max())  # max label class\n","  assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data_folder}. Possible class labels are 0-{nc - 1}'\n","\n","  # Model attributes\n","  nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)\n","  hyp['box'] *= 3 / nl  # scale to layers\n","  hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers\n","  hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n","  hyp['label_smoothing'] = label_smoothing\n","  model.nc = nc  # attach number of classes to model\n","  model.hyp = hyp  # attach hyperparameters to model\n","  model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n","  model.names = names\n","\n","  # init YOLO loss\n","  compute_loss = ComputeIndividualLoss(model)\n","\n","  attacker = FGSMAttacker(epsilon, device, att_type)\n","\n","  model.train()\n","\n","  pbar = enumerate(train_loader)\n","  pbar = tqdm(pbar, total=len(train_loader), bar_format=TQDM_BAR_FORMAT)  # progress bar\n","  # optimizer.zero_grad() ??????\n","  adv_images = []\n","  original_images = []\n","  adv_loss = 0.0\n","  for batch_i, (im, targets, paths, shapes) in (pbar):\n","      if batch_i == num_batches:\n","        break\n","      if cuda:\n","          im = im.to(device, non_blocking=True).float() / 255\n","          targets = targets.to(device)\n","      nb, _, height, width = im.shape  # batch size, channels, height, width\n","\n","      adv_image = attacker.attack(im, targets, compute_loss, model)\n","      # evaluate model's loss\n","      with torch.no_grad():\n","          if cuda:\n","              adv_image = adv_image.to(device)\n","          adv_pred = model(adv_image)\n","          loss_total, _, _, _, _ = compute_loss(\n","              adv_pred, targets\n","          )\n","          adv_loss += loss_total.cpu().item()\n","\n","      adv_images.append(adv_image)\n","      original_images.append(im)\n","  return torch.cat(adv_images), torch.cat(original_images), adv_loss / len(pbar)"],"metadata":{"id":"986KWXRhRJr6","executionInfo":{"status":"ok","timestamp":1690919241701,"user_tz":240,"elapsed":985,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["adv_examples_cls, orig_examples_cls, mean_loss_cls = get_adv_examples(6.0, \"/content/bestC.pt\", 10, \"cls\", \"/content/yolov5/APS360_(2)-7/data.yaml\", 'data/hyps/hyp.scratch-low.yaml', device=torch.device(\"cuda:0\"), batch_size=4)\n","adv_examples_obj, orig_examples_obj, mean_loss_obj = get_adv_examples(6.0, \"/content/bestC.pt\", 10, \"conf\", \"/content/yolov5/APS360_(2)-7/data.yaml\", 'data/hyps/hyp.scratch-low.yaml', device=torch.device(\"cuda:0\"), batch_size=4)\n","adv_examples_loc, orig_examples_loc, mean_loss_loc = get_adv_examples(6.0, \"/content/bestC.pt\", 10, \"loc\", \"/content/yolov5/APS360_(2)-7/data.yaml\", 'data/hyps/hyp.scratch-low.yaml', device=torch.device(\"cuda:0\"), batch_size=4)\n","adv_examples_t, orig_examples_t, mean_loss_t = get_adv_examples(3.0, \"/content/bestC.pt\", 1, \"total\", \"/content/yolov5/APS360_(2)-7/data.yaml\", 'data/hyps/hyp.scratch-low.yaml', device=torch.device(\"cuda:0\"), batch_size=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNA9afcPSj-u","executionInfo":{"status":"ok","timestamp":1690919260602,"user_tz":240,"elapsed":16455,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"7939afe5-24d9-4d6a-fde2-a59d6f8315e9"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     28287  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","Model summary: 291 layers, 20875359 parameters, 20875359 gradients, 48.2 GFLOPs\n","\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"]},{"output_type":"stream","name":"stdout","text":["Transferred 481/481 items from /content/bestC.pt\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/APS360_(2)-7/train/labels.cache... 1766 images, 28 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1766/1766 [00:00<?, ?it/s]\n","  2%|â–         | 10/442 [00:03<02:27,  2.92it/s]\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     28287  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","Model summary: 291 layers, 20875359 parameters, 20875359 gradients, 48.2 GFLOPs\n","\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"]},{"output_type":"stream","name":"stdout","text":["Transferred 481/481 items from /content/bestC.pt\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/APS360_(2)-7/train/labels.cache... 1766 images, 28 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1766/1766 [00:00<?, ?it/s]\n","  2%|â–         | 10/442 [00:03<02:30,  2.87it/s]\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     28287  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","Model summary: 291 layers, 20875359 parameters, 20875359 gradients, 48.2 GFLOPs\n","\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"]},{"output_type":"stream","name":"stdout","text":["Transferred 481/481 items from /content/bestC.pt\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/APS360_(2)-7/train/labels.cache... 1766 images, 28 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1766/1766 [00:00<?, ?it/s]\n","  2%|â–         | 10/442 [00:03<02:26,  2.96it/s]\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     28287  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","Model summary: 291 layers, 20875359 parameters, 20875359 gradients, 48.2 GFLOPs\n","\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"]},{"output_type":"stream","name":"stdout","text":["Transferred 481/481 items from /content/bestC.pt\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/APS360_(2)-7/train/labels.cache... 1766 images, 28 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1766/1766 [00:00<?, ?it/s]\n","  0%|          | 1/442 [00:00<03:02,  2.42it/s]\n"]}]},{"cell_type":"code","source":["print(mean_loss_t)\n","print(mean_loss_obj)\n","print(mean_loss_loc)\n","print(mean_loss_cls)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf-J6hOKdsrg","executionInfo":{"status":"ok","timestamp":1690919260603,"user_tz":240,"elapsed":27,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"dd6a7eab-1730-4e6e-e23e-55260f5bcdea"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["0.000136911397066591\n","0.001612100834490487\n","0.0013993959873914719\n","0.001595092566506895\n"]}]},{"cell_type":"code","source":["# save them as png files\n","# !mkdir /content/adv_examples\n","!mkdir /content/adv_examples/try_3_obj\n","for i in range(len(adv_examples_obj)):\n","  plt.imshow(adv_examples_obj[i].detach().cpu().permute((1,2,0)))\n","  plt.savefig(f\"/content/adv_examples/try_3_obj/example_{i}.png\")"],"metadata":{"id":"zO0dwy6Jiuco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save them as png files\n","# !mkdir /content/orig_examples\n","!mkdir /content/orig_examples/try_3_obj\n","for i in range(len(orig_examples_obj)):\n","  plt.imshow(orig_examples_obj[i].detach().cpu().permute((1,2,0)))\n","  plt.savefig(f\"/content/orig_examples/try_3_obj/example_{i}.png\")"],"metadata":{"id":"oLktEdD5l1Be","executionInfo":{"status":"ok","timestamp":1690920430895,"user_tz":240,"elapsed":528469,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["!python detect.py --source /content/adv_examples/try_3_obj --weights /content/bestC.pt --name adv_example_run_4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiaPQmCqmAcw","executionInfo":{"status":"ok","timestamp":1690920438169,"user_tz":240,"elapsed":7276,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"a522416f-bae5-4c75-f876-fce5bec90b14"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/bestC.pt'], source=/content/adv_examples/try_3_obj, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=adv_example_run_4, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 ğŸš€ v7.0-200-g05e4c05 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n","image 1/40 /content/adv_examples/try_3_obj/example_0.png: 480x640 1 seatbelt, 42.8ms\n","image 2/40 /content/adv_examples/try_3_obj/example_1.png: 480x640 1 noSeatbelt, 21.4ms\n","image 3/40 /content/adv_examples/try_3_obj/example_10.png: 480x640 1 noSeatbelt, 21.3ms\n","image 4/40 /content/adv_examples/try_3_obj/example_11.png: 480x640 2 seatbelts, 21.3ms\n","image 5/40 /content/adv_examples/try_3_obj/example_12.png: 480x640 1 seatbelt, 21.3ms\n","image 6/40 /content/adv_examples/try_3_obj/example_13.png: 480x640 1 noSeatbelt, 21.4ms\n","image 7/40 /content/adv_examples/try_3_obj/example_14.png: 480x640 1 noSeatbelt, 21.3ms\n","image 8/40 /content/adv_examples/try_3_obj/example_15.png: 480x640 1 seatbelt, 21.3ms\n","image 9/40 /content/adv_examples/try_3_obj/example_16.png: 480x640 1 seatbelt, 18.0ms\n","image 10/40 /content/adv_examples/try_3_obj/example_17.png: 480x640 1 seatbelt, 17.9ms\n","image 11/40 /content/adv_examples/try_3_obj/example_18.png: 480x640 1 noSeatbelt, 1 seatbelt, 17.9ms\n","image 12/40 /content/adv_examples/try_3_obj/example_19.png: 480x640 1 seatbelt, 17.9ms\n","image 13/40 /content/adv_examples/try_3_obj/example_2.png: 480x640 1 noSeatbelt, 17.9ms\n","image 14/40 /content/adv_examples/try_3_obj/example_20.png: 480x640 1 seatbelt, 16.3ms\n","image 15/40 /content/adv_examples/try_3_obj/example_21.png: 480x640 1 noSeatbelt, 16.0ms\n","image 16/40 /content/adv_examples/try_3_obj/example_22.png: 480x640 1 seatbelt, 16.0ms\n","image 17/40 /content/adv_examples/try_3_obj/example_23.png: 480x640 1 seatbelt, 16.0ms\n","image 18/40 /content/adv_examples/try_3_obj/example_24.png: 480x640 (no detections), 16.0ms\n","image 19/40 /content/adv_examples/try_3_obj/example_25.png: 480x640 1 seatbelt, 16.0ms\n","image 20/40 /content/adv_examples/try_3_obj/example_26.png: 480x640 1 seatbelt, 14.6ms\n","image 21/40 /content/adv_examples/try_3_obj/example_27.png: 480x640 1 seatbelt, 14.6ms\n","image 22/40 /content/adv_examples/try_3_obj/example_28.png: 480x640 1 noSeatbelt, 14.5ms\n","image 23/40 /content/adv_examples/try_3_obj/example_29.png: 480x640 1 noSeatbelt, 1 seatbelt, 14.4ms\n","image 24/40 /content/adv_examples/try_3_obj/example_3.png: 480x640 1 noSeatbelt, 14.4ms\n","image 25/40 /content/adv_examples/try_3_obj/example_30.png: 480x640 1 noSeatbelt, 14.3ms\n","image 26/40 /content/adv_examples/try_3_obj/example_31.png: 480x640 2 seatbelts, 14.3ms\n","image 27/40 /content/adv_examples/try_3_obj/example_32.png: 480x640 1 noSeatbelt, 14.3ms\n","image 28/40 /content/adv_examples/try_3_obj/example_33.png: 480x640 1 noSeatbelt, 14.3ms\n","image 29/40 /content/adv_examples/try_3_obj/example_34.png: 480x640 1 noSeatbelt, 14.3ms\n","image 30/40 /content/adv_examples/try_3_obj/example_35.png: 480x640 1 noSeatbelt, 14.3ms\n","image 31/40 /content/adv_examples/try_3_obj/example_36.png: 480x640 1 seatbelt, 14.6ms\n","image 32/40 /content/adv_examples/try_3_obj/example_37.png: 480x640 1 seatbelt, 14.6ms\n","image 33/40 /content/adv_examples/try_3_obj/example_38.png: 480x640 1 noSeatbelt, 14.2ms\n","image 34/40 /content/adv_examples/try_3_obj/example_39.png: 480x640 1 noSeatbelt, 14.1ms\n","image 35/40 /content/adv_examples/try_3_obj/example_4.png: 480x640 1 seatbelt, 14.1ms\n","image 36/40 /content/adv_examples/try_3_obj/example_5.png: 480x640 1 seatbelt, 14.0ms\n","image 37/40 /content/adv_examples/try_3_obj/example_6.png: 480x640 1 noSeatbelt, 14.1ms\n","image 38/40 /content/adv_examples/try_3_obj/example_7.png: 480x640 1 noSeatbelt, 14.1ms\n","image 39/40 /content/adv_examples/try_3_obj/example_8.png: 480x640 1 noSeatbelt, 14.1ms\n","image 40/40 /content/adv_examples/try_3_obj/example_9.png: 480x640 1 noSeatbelt, 14.1ms\n","Speed: 0.4ms pre-process, 17.0ms inference, 3.6ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1mruns/detect/adv_example_run_4\u001b[0m\n"]}]},{"cell_type":"code","source":["!python detect.py --source /content/orig_examples/try_3_obj --weights /content/bestC.pt --name orig_example_run_4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vU9DP90KfmXM","executionInfo":{"status":"ok","timestamp":1690920446474,"user_tz":240,"elapsed":8324,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"5456265e-b856-44aa-c80a-062b2e33f4e2"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/bestC.pt'], source=/content/orig_examples/try_3_obj, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=orig_example_run_4, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 ğŸš€ v7.0-200-g05e4c05 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n","image 1/40 /content/orig_examples/try_3_obj/example_0.png: 480x640 2 seatbelts, 46.1ms\n","image 2/40 /content/orig_examples/try_3_obj/example_1.png: 480x640 1 noSeatbelt, 21.4ms\n","image 3/40 /content/orig_examples/try_3_obj/example_10.png: 480x640 1 noSeatbelt, 21.3ms\n","image 4/40 /content/orig_examples/try_3_obj/example_11.png: 480x640 2 seatbelts, 21.3ms\n","image 5/40 /content/orig_examples/try_3_obj/example_12.png: 480x640 1 seatbelt, 21.3ms\n","image 6/40 /content/orig_examples/try_3_obj/example_13.png: 480x640 1 noSeatbelt, 21.3ms\n","image 7/40 /content/orig_examples/try_3_obj/example_14.png: 480x640 1 noSeatbelt, 21.3ms\n","image 8/40 /content/orig_examples/try_3_obj/example_15.png: 480x640 1 seatbelt, 21.3ms\n","image 9/40 /content/orig_examples/try_3_obj/example_16.png: 480x640 1 seatbelt, 21.3ms\n","image 10/40 /content/orig_examples/try_3_obj/example_17.png: 480x640 1 noSeatbelt, 17.0ms\n","image 11/40 /content/orig_examples/try_3_obj/example_18.png: 480x640 1 noSeatbelt, 1 seatbelt, 17.0ms\n","image 12/40 /content/orig_examples/try_3_obj/example_19.png: 480x640 1 seatbelt, 17.0ms\n","image 13/40 /content/orig_examples/try_3_obj/example_2.png: 480x640 1 noSeatbelt, 17.0ms\n","image 14/40 /content/orig_examples/try_3_obj/example_20.png: 480x640 1 seatbelt, 17.0ms\n","image 15/40 /content/orig_examples/try_3_obj/example_21.png: 480x640 1 noSeatbelt, 16.3ms\n","image 16/40 /content/orig_examples/try_3_obj/example_22.png: 480x640 1 seatbelt, 15.1ms\n","image 17/40 /content/orig_examples/try_3_obj/example_23.png: 480x640 1 seatbelt, 15.1ms\n","image 18/40 /content/orig_examples/try_3_obj/example_24.png: 480x640 1 noSeatbelt, 15.1ms\n","image 19/40 /content/orig_examples/try_3_obj/example_25.png: 480x640 1 seatbelt, 15.1ms\n","image 20/40 /content/orig_examples/try_3_obj/example_26.png: 480x640 1 seatbelt, 15.1ms\n","image 21/40 /content/orig_examples/try_3_obj/example_27.png: 480x640 1 seatbelt, 14.2ms\n","image 22/40 /content/orig_examples/try_3_obj/example_28.png: 480x640 1 noSeatbelt, 14.2ms\n","image 23/40 /content/orig_examples/try_3_obj/example_29.png: 480x640 1 noSeatbelt, 1 seatbelt, 14.2ms\n","image 24/40 /content/orig_examples/try_3_obj/example_3.png: 480x640 1 noSeatbelt, 13.9ms\n","image 25/40 /content/orig_examples/try_3_obj/example_30.png: 480x640 1 noSeatbelt, 13.9ms\n","image 26/40 /content/orig_examples/try_3_obj/example_31.png: 480x640 2 seatbelts, 13.9ms\n","image 27/40 /content/orig_examples/try_3_obj/example_32.png: 480x640 1 noSeatbelt, 13.7ms\n","image 28/40 /content/orig_examples/try_3_obj/example_33.png: 480x640 1 noSeatbelt, 13.7ms\n","image 29/40 /content/orig_examples/try_3_obj/example_34.png: 480x640 1 noSeatbelt, 13.6ms\n","image 30/40 /content/orig_examples/try_3_obj/example_35.png: 480x640 1 noSeatbelt, 13.4ms\n","image 31/40 /content/orig_examples/try_3_obj/example_36.png: 480x640 1 seatbelt, 13.4ms\n","image 32/40 /content/orig_examples/try_3_obj/example_37.png: 480x640 1 seatbelt, 13.4ms\n","image 33/40 /content/orig_examples/try_3_obj/example_38.png: 480x640 1 noSeatbelt, 13.4ms\n","image 34/40 /content/orig_examples/try_3_obj/example_39.png: 480x640 1 noSeatbelt, 13.4ms\n","image 35/40 /content/orig_examples/try_3_obj/example_4.png: 480x640 1 seatbelt, 13.4ms\n","image 36/40 /content/orig_examples/try_3_obj/example_5.png: 480x640 1 seatbelt, 13.4ms\n","image 37/40 /content/orig_examples/try_3_obj/example_6.png: 480x640 1 noSeatbelt, 13.5ms\n","image 38/40 /content/orig_examples/try_3_obj/example_7.png: 480x640 1 noSeatbelt, 13.3ms\n","image 39/40 /content/orig_examples/try_3_obj/example_8.png: 480x640 1 noSeatbelt, 13.3ms\n","image 40/40 /content/orig_examples/try_3_obj/example_9.png: 480x640 1 noSeatbelt, 13.3ms\n","Speed: 0.4ms pre-process, 16.7ms inference, 2.9ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1mruns/detect/orig_example_run_4\u001b[0m\n"]}]},{"cell_type":"code","source":["import shutil\n","\n","# Specify the folder path you want to delete\n","# folder_path = '/content/original_examples'  # Replace this with the actual folder path\n","folder_path = '/content/adv_examples'  # Replace this with the actual folder path\n","\n","# Use shutil.rmtree() to recursively remove the folder and its contents\n","shutil.rmtree(folder_path)"],"metadata":{"id":"t8BX4btMniEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# zip the adversarial examples to be saved for training\n","import shutil\n","\n","folder_path = \"/content/yolov5/runs/detect/adv_example_run_4\"\n","zip_path = \"/content/yolov5/runs/detect/adv_example_run_4/zipfile.zip\"\n","\n","shutil.make_archive(zip_path.split(\".\")[0], 'zip', folder_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":550},"id":"lJ9t2CVTk3yY","executionInfo":{"status":"error","timestamp":1690921624992,"user_tz":240,"elapsed":1113510,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"de14502e-b238-406a-df9f-f56c046bd25e"},"execution_count":41,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1775\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mfdst_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-f04c66d83546>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/yolov5/runs/detect/adv_example_run_1/zipfile.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_cwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                         \u001b[0marcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marcdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1773\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zip64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mZIP64_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                         raise RuntimeError(\n\u001b[0m\u001b[1;32m   1171\u001b[0m                             'File size unexpectedly exceeded ZIP64 limit')\n\u001b[1;32m   1172\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mZIP64_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: File size unexpectedly exceeded ZIP64 limit"]}]}]}