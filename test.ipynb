{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyONP4LfAWfMRv4yn983dXKK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# set up"],"metadata":{"id":"5mQNmpvkLq80"}},{"cell_type":"code","source":["# clone yolov5 repo\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow\n","%pip install loguru"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-LlC1EOLeqO","executionInfo":{"status":"ok","timestamp":1690861185587,"user_tz":240,"elapsed":11967,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"dc5cddb5-3bce-4581-86c1-fee28a11417d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15845, done.\u001b[K\n","remote: Counting objects: 100% (77/77), done.\u001b[K\n","remote: Compressing objects: 100% (64/64), done.\u001b[K\n","remote: Total 15845 (delta 31), reused 39 (delta 13), pack-reused 15768\u001b[K\n","Receiving objects: 100% (15845/15845), 14.66 MiB | 20.99 MiB/s, done.\n","Resolving deltas: 100% (10843/10843), done.\n","/content/yolov5\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.6/605.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKLxvhNhHbmB","executionInfo":{"status":"ok","timestamp":1690860443037,"user_tz":240,"elapsed":15771,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"72b21181-7413-4873-d1c9-c6eb58012d3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.2)\n","Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2022.12.7)\n","Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (8.4.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.27.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.16)\n","Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.2)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.65.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.41.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (2.0.12)\n","Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (4.8.0.74)\n","loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in Test-Set-1 to yolov5pytorch: 100% [22466926 / 22466926] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to Test-Set-1 in yolov5pytorch:: 100%|██████████| 966/966 [00:00<00:00, 1860.81it/s]\n"]}],"source":["from roboflow import Roboflow\n","rf = Roboflow(api_key=\"gMTVjr2tVu8feJmkrnPs\")\n","project = rf.workspace(\"aps360-project-6tmc3\").project(\"test-set-r5re0\")\n","dataset = project.version(1).download(\"yolov5\")"]},{"cell_type":"code","source":["from utils.torch_utils import select_device, smart_inference_mode\n","from models.yolo import Model\n","from utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,\n","                           check_git_status, check_img_size, check_requirements, check_suffix, check_yaml, colorstr,\n","                           get_latest_run, increment_path, init_seeds, intersect_dicts, labels_to_class_weights,\n","                           labels_to_image_weights, methods, one_cycle, print_args, print_mutation, strip_optimizer,\n","                           yaml_save)\n","from utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, select_device, smart_DDP, smart_optimizer,\n","                               smart_resume, torch_distributed_zero_first)\n","\n","from utils.loss import ComputeLoss\n","import torch\n","import os\n","import yaml\n","LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n","RANK = int(os.getenv('RANK', -1))\n","WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from utils.dataloaders import create_dataloader\n","from tqdm import tqdm\n","from loguru import logger\n","from utils.metrics import bbox_iou\n"],"metadata":{"id":"RT5tkwbrLmsH","executionInfo":{"status":"ok","timestamp":1690866029814,"user_tz":240,"elapsed":94,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def _load_model(weights_path, device, nc, anchors):\n","  '''\n","    @param data_folder: path to the data.yaml file\n","    return a model that is loaded with the specified weights\n","  '''\n","  ckpt = torch.load(weights_path, map_location=\"cpu\")\n","  model = Model(ckpt['model'].yaml, ch=3, nc=nc, anchors=anchors).to(device)  # create\n","  exclude = []\n","  # exclude = ['anchor'] if anchors else []\n","  csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n","  csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n","  model.load_state_dict(csd, strict=False)  # load\n","  print(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights_path}')  # report)\n","  return model"],"metadata":{"id":"5MsQPkO_MhJO","executionInfo":{"status":"ok","timestamp":1690861445260,"user_tz":240,"elapsed":98,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class ComputeIndividualLoss(ComputeLoss):\n","  def __init__(self, model, autobalance=False):\n","    super().__init__(model, autobalance)\n","\n","  # override the __call__ function to return the component loss tensors\n","  def __call__(self, p, targets):  # predictions, targets\n","    lcls = torch.zeros(1, device=self.device)  # class loss\n","    lbox = torch.zeros(1, device=self.device)  # box loss\n","    lobj = torch.zeros(1, device=self.device)  # object loss\n","    tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n","\n","    # Losses\n","    for i, pi in enumerate(p):  # layer index, layer predictions\n","        b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n","        tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj\n","\n","        n = b.shape[0]  # number of targets\n","        if n:\n","            # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0\n","            pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions\n","\n","            # Regression\n","            pxy = pxy.sigmoid() * 2 - 0.5\n","            pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i]\n","            pbox = torch.cat((pxy, pwh), 1)  # predicted box\n","            iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)\n","            lbox += (1.0 - iou).mean()  # iou loss\n","\n","            # Objectness\n","            iou = iou.detach().clamp(0).type(tobj.dtype)\n","            if self.sort_obj_iou:\n","              j = iou.argsort()\n","              b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]\n","            if self.gr < 1:\n","                iou = (1.0 - self.gr) + self.gr * iou\n","            tobj[b, a, gj, gi] = iou  # iou ratio\n","\n","            # Classification\n","            if self.nc > 1:  # cls loss (only if multiple classes)\n","                t = torch.full_like(pcls, self.cn, device=self.device)  # targets\n","                t[range(n), tcls[i]] = self.cp\n","                lcls += self.BCEcls(pcls, t)  # BCE\n","            # Append targets to text file\n","            # with open('targets.txt', 'a') as file:\n","            #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\n","\n","        obji = self.BCEobj(pi[..., 4], tobj)\n","        lobj += obji * self.balance[i]  # obj loss\n","        if self.autobalance:\n","            self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\n","\n","    if self.autobalance:\n","        self.balance = [x / self.balance[self.ssi] for x in self.balance]\n","    lbox *= self.hyp['box']\n","    lobj *= self.hyp['obj']\n","    lcls *= self.hyp['cls']\n","    bs = tobj.shape[0]  # batch size\n","\n","    return (lbox + lobj + lcls), torch.cat((lbox, lobj, lcls)).detach(), lbox, lobj, lcls"],"metadata":{"id":"XffA-ALmV7JY","executionInfo":{"status":"ok","timestamp":1690866395863,"user_tz":240,"elapsed":81,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def evaluate(weights_path,\n","              data_folder,\n","              hyperparam_path,\n","              batch_size=32,\n","              imgsz=640,\n","              device='',\n","              seed=0,\n","              workers=2,\n","              label_smoothing=0.0):\n","\n","  single_cls = False\n","  # load in the hyperparameters\n","  if isinstance(hyperparam_path, str):\n","        with open(hyperparam_path, errors='ignore') as f:\n","            hyp = yaml.safe_load(f)  # load hyps dict\n","\n","  cuda = device.type != 'cpu'\n","  init_seeds(seed + 1 + RANK, deterministic=True)\n","\n","  # load data dict\n","  with torch_distributed_zero_first(LOCAL_RANK):\n","      data_dict = check_dataset(data_folder)  # check if None\n","  nc = int(data_dict[\"nc\"])\n","  names = data_dict[\"names\"]\n","  is_coco = False\n","\n","  # load model with weights\n","  model = _load_model(weights_path, device, nc, hyp.get(\"anchors\"))\n","  amp = check_amp(model)  # check AMP\n","\n","  gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n","  imgsz = check_img_size(imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n","\n","  ema = ModelEMA(model) if RANK in {-1, 0} else None\n","\n","  test_path = data_dict['test']\n","  test_loader, dataset = create_dataloader(test_path,\n","                                            imgsz,\n","                                            batch_size // WORLD_SIZE,\n","                                            gs,\n","                                            single_cls,\n","                                            hyp=hyp,\n","                                            augment=False,\n","                                            cache=None,\n","                                            rect=False, #unless you specify this flag when training\n","                                            rank=LOCAL_RANK,\n","                                            workers=workers,\n","                                            image_weights=False, # unless you specify this flag when training\n","                                            quad=False, # unless you specify this flag when training\n","                                            prefix=colorstr('test: '),\n","                                            shuffle=True,\n","                                            seed=seed)\n","  labels = np.concatenate(dataset.labels, 0)\n","  mlc = int(labels[:, 0].max())  # max label class\n","  assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data_folder}. Possible class labels are 0-{nc - 1}'\n","\n","  # Model attributes\n","  nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)\n","  hyp['box'] *= 3 / nl  # scale to layers\n","  hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers\n","  hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n","  hyp['label_smoothing'] = label_smoothing\n","  model.nc = nc  # attach number of classes to model\n","  model.hyp = hyp  # attach hyperparameters to model\n","  model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n","  model.names = names\n","\n","  model.train()\n","  compute_loss = ComputeIndividualLoss(model)\n","  pbar = enumerate(test_loader)\n","  pbar = tqdm(pbar, total=len(test_loader), bar_format=TQDM_BAR_FORMAT)  # progress bar\n","\n","  for batch_i, (im, targets, paths, shapes) in (pbar):\n","      if cuda:\n","          im = im.to(device, non_blocking=True).float() / 255\n","          targets = targets.to(device)\n","      nb, _, height, width = im.shape  # batch size, channels, height, width\n","      with torch.no_grad():\n","          pred = model(im)\n","\n","          compute_loss(pred, targets)"],"metadata":{"id":"whOmaDuwLo8-","executionInfo":{"status":"ok","timestamp":1690866398828,"user_tz":240,"elapsed":134,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["evaluate(\"/content/bestC.pt\", \"/content/yolov5/Test-Set-1/data.yaml\", 'data/hyps/hyp.scratch-low.yaml', device=torch.device(\"cuda:0\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Xokxq5BdIJR0","executionInfo":{"status":"error","timestamp":1690866424837,"user_tz":240,"elapsed":23177,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}},"outputId":"a2c85113-dced-4b64-de17-95c1b843e047"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     28287  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","Model summary: 291 layers, 20875359 parameters, 20875359 gradients, 48.2 GFLOPs\n","\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"output_type":"stream","name":"stdout","text":["Transferred 481/481 items from /content/bestC.pt\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/yolov5/Test-Set-1/test/labels.cache... 480 images, 1 backgrounds, 0 corrupt: 100%|██████████| 480/480 [00:00<?, ?it/s]\n","  0%|          | 0/15 [00:00<?, ?it/s]\u001b[32m2023-08-01 05:07:04.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mlen of 3\u001b[0m\n","\u001b[32m2023-08-01 05:07:04.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mt shape torch.Size([29, 2])\u001b[0m\n","\u001b[32m2023-08-01 05:07:04.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mpcls shape torch.Size([29, 2])\u001b[0m\n","\u001b[32m2023-08-01 05:07:04.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mt tensor([[1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.],\n","        [1., 0.]], device='cuda:0')\u001b[0m\n","\u001b[32m2023-08-01 05:07:04.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mpcls tensor([[ 5.14684, -5.42702],\n","        [ 6.02857, -5.88305],\n","        [ 0.71413, -0.71929],\n","        [ 3.46399, -3.48552],\n","        [ 4.35371, -4.44581],\n","        [ 5.81710, -5.92201],\n","        [ 5.47139, -5.52441],\n","        [ 3.22228, -3.34726],\n","        [ 3.57276, -3.63015],\n","        [ 0.36156, -0.42097],\n","        [ 5.32852, -5.68832],\n","        [ 6.20921, -6.20275],\n","        [ 3.26509, -3.27976],\n","        [ 2.95135, -3.02298],\n","        [ 3.12585, -3.19637],\n","        [ 4.04678, -4.12921],\n","        [ 0.14609, -0.20595],\n","        [ 0.91481, -0.95284],\n","        [ 3.74812, -3.87159],\n","        [ 5.70613, -5.67364],\n","        [ 4.94519, -5.08683],\n","        [ 4.53807, -5.00757],\n","        [ 5.45510, -5.42230],\n","        [ 0.06985, -0.16898],\n","        [ 3.48836, -3.50621],\n","        [ 3.29805, -3.44435],\n","        [ 5.23700, -5.36741],\n","        [ 4.20095, -4.24392],\n","        [ 3.02910, -3.03353]], device='cuda:0')\u001b[0m\n","  0%|          | 0/15 [00:01<?, ?it/s]\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-a475fdc1f280>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/bestC.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/yolov5/Test-Set-1/data.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/hyps/hyp.scratch-low.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-74dcec2058fd>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(weights_path, data_folder, hyperparam_path, batch_size, imgsz, device, seed, workers, label_smoothing)\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m           \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-a30095acff1c>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, p, targets)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"t {t}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pcls {pcls}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Append targets to text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# with open('targets.txt', 'a') as file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"markdown","source":["# baseline model"],"metadata":{"id":"sD2SIw6cOqi9"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"Ct5hn2Q2O1Fi","executionInfo":{"status":"ok","timestamp":1690862069940,"user_tz":240,"elapsed":79,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class BaselineNet(nn.Module):\n","    def __init__(self):\n","        super(BaselineNet, self).__init__()\n","        self.name = \"baseline\"\n","        self.conv = nn.Conv2d(3, 5, 5, stride = 2)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(4500, 5 * 7 * 7)\n","        self.fc2 = nn.Linear(5 * 7 * 7, 1)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv(x)))\n","        x = self.pool(x)\n","        x = x.view(-1, 4500)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        x = x.squeeze(1) # Flatten to [batch_size]\n","        return x"],"metadata":{"id":"C-2tSzigOsIW","executionInfo":{"status":"ok","timestamp":1690862070770,"user_tz":240,"elapsed":93,"user":{"displayName":"Ziyu zhang","userId":"08758267182289410254"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["baseline_model = BaselineNet()\n"],"metadata":{"id":"QOcxIDOXO7nw"},"execution_count":null,"outputs":[]}]}